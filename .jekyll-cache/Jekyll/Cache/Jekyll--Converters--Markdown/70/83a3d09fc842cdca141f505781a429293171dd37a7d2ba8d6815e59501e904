I"á-<h3 id="work-presented-at-neurips-2018-workshop-latinx-in-ai">Work presented at NeurIPS 2018, Workshop LatinX in AI</h3>

<p>Our world is hierarchical and compositional, humans can generalise better since we use primitive
concepts that allow us to create complex representations (Higgins et al. (2016)).<br />
Towards the creation of truly intelligent systems, they should learn in a similar way resulting in an 
increase of their performance since they would capture the underlying factors of variation of the data 
( Bengio et al.(2013); Hassabis et al. (2017)).</p>

<p>According to Lake et al. (2016), a compositional representation should create new elements from 
the combination of primitive concepts resulting in a infinite number of new representations.  For 
example if our model is trained with images of white wall and then is presented a boy with a white 
shirt, it should identify the color white as a primitive element.</p>

<p>Furthermore, a disentangled representation has been interpreted in different ways, for instance Bengio et al. (2013) 
define it as one where single latent variables are sensitive to changes in generative factors, while being 
invariant to changes in other factors.</p>

<p>The original Variational auto-encoder (VAE) framework (Kingma &amp; Welling (2013); Rezende et al.(2014)) has been used 
extensively for the task of disentanglement by modifying the original ELBO formulation; for instance Œ≤-VAE, presented in Higgins et al. (2017a), 
increases the latent capacity by penalising the KL divergence term with a Œ≤ hyperparameter.</p>

<script type="math/tex; mode=display">\mathcal{L}_{elbo}(\theta,\phi,x) =  E_{q_{\phi}(z | x)} \big[\ \log p_{\theta}(x | z) \big]\ - KL \big[\ q_{\phi}(z | x) \Vert p(z) \big]\</script>

<script type="math/tex; mode=display">\mathcal{L}_{\beta-vae}(\theta,\phi,x) =  E_{q_{\phi}(z | x)} \big[\ \log p_{\theta}(x | z) \big]\ - \beta * KL \big[\ q_{\phi}(z | x) \Vert p(z) \big]\</script>

<p>We proposed a new approach for the task of disentanglement, where we achieve state-of-the-art results by penalizing the synergistic mutual 
information within the latents we encourage information independence and by doing that disentangle the latent factors. It is worth noting that our model draws inspiration
from population coding, a field from neuroscience, where the notion of synergy arises.</p>

<h2 id="information-theory">Information Theory</h2>

<p>First of all, we need a brief introduction to the field of information theory in order to understand the partial information decomposition, which it will lead us 
to the importance of Synergy in our framework. First, we define the Mutual Information:</p>

<script type="math/tex; mode=display">\begin{equation}
I(X;Y) = E_{p(x,y)} \bigg[\ \log \frac{p(x,y)}{p(x)p(y)}  \bigg]\ = K \big[\ p(x,y) \Vert p(x)p(y) \big]\
\end{equation}</script>

<p>Now, we are going to introduce the Partial Information decomposition; let‚Äôs consider the random variable S and a random vector <script type="math/tex">R = \{R_{1}, R_{2}, .., R_{n}\}</script>, being our goal to 
decompose the information that the variable $R$ provides about S; the contribution of these partial information 
could come from one element from <script type="math/tex">R_{1}</script> or from subsets of $R$ (ie. $R_{1}, R_{2}$). Considering the case with 
two variables, <script type="math/tex">\{R_{1}, R_{2}\}</script>, we could separate the partial mutual information in unique (<script type="math/tex">Unq(S; R_{1})</script> 
and <script type="math/tex">Unq(S; R_{2})</script> ), the information that only <script type="math/tex">R_{1}</script> or <script type="math/tex">R_{2}</script> provides about S; redundant (<script type="math/tex">Rdn(S; R_{1}, R_{2})</script>), 
which it could be provided by <script type="math/tex">R_{1}</script> or <script type="math/tex">R_{2}</script>; and synergistic (<script type="math/tex">Syn(S; R_{1}, R_{2})</script>), which is only 
provided by the combination of <script type="math/tex">R_{1}</script> and <script type="math/tex">R_{2}</script>. The figure 2.11 depicts the decomposition; this diagram 
is also called PI-diagram (Partial information) . Additional notation is used for a better visualisation in the 
case of more variables: Unique <script type="math/tex">\{1\}</script>,<script type="math/tex">\{2\}</script>; Redundant <script type="math/tex">\{1\}\{2\}</script> and Synergistic <script type="math/tex">\{12\}</script>.</p>

<h2 id="synergy">Synergy</h2>

<p>Synergy arises in different fields, being a popular notion of it as how much the whole is greater than the sum of its parts.</p>

<script type="math/tex; mode=display">\begin{equation}
I(S; R_{1},R_{2}) = \underbrace{SI(S;R_{1},R_{2})}_\text{Redundant} + \underbrace{Unq(S;R_{1} \setminus{R_{2}})}_\text{Unique} + \underbrace{Unq(S;R_{2} \setminus{R_{1}})}_\text{Unique} + \underbrace{Syn(S;R_{1},R_{2})}_\text{Synergistic}
\end{equation}</script>

<p><img src="http://localhost:4000/images/2_variables.png" alt="results" /></p>

<p>For neural codes there are three types of independence when it comes to the relation between stimuli
and responses; which are the activity independence, the conditional independence and the information 
independence.  One of the first measures of synergy for sets of sources of information came from 
this notion of independence. In Williams &amp; Beer (2010) it is stated that if the responses come 
from different features of the stimulus, the information encoded in those responses should be added 
to estimate the mutual information they provide about the stimulus. Formally:</p>

<p><img src="http://localhost:4000/images/population.png" alt="population coding" /></p>

<p>The intuition behind this metric is that synergy should be defined as the ‚Äúwhole beyond the 
maximum of its parts‚Äù. The whole is described as the mutual information between the joint $\textbf{X}$ 
and the outcome Y; whereas the maximum of all the possible subsets is interpreted as the maximum information 
that any of the sources $\sA_{i}$ provided about each outcome. Formally, this is stated as:</p>

<p>However, we just saw in the previous sections that theI(S;R1)andI(S;R2)could be decomposed 
in their unique and redundant and synergistic terms. Intuitively, this formulation only holds if there 
is no redundant or synergistic information present; which means in the context of population coding 
that the responses encoded different parts of the stimulus. If the responses R1 and R2 convey more 
information together than separate, we can say we have synergistic information; if the information 
is less, we have redundant information. That‚Äôs the reason why in Gat &amp; Tishby (1998), the synergy 
is considered as measure of information independence</p>

<script type="math/tex; mode=display">\begin{equation}
Syn(R_{1}, R_{2}) = I(S; R_{1}, R_{2}) - I(S;R_{1}) - I(S;R_{2})
\end{equation}</script>

<p><img src="http://localhost:4000/images/synergy_3_latens.png" alt="results" /></p>

<ul>
  <li>n: Number of individual predictors <script type="math/tex">X_{i}</script></li>
  <li><script type="math/tex">\mathbb{A}_{i}</script> : subset of individual predictors (ie. <script type="math/tex">A_{i} = \{X_{1},X_{3}\}</script>)</li>
  <li><strong>X</strong>: Joint random variable of all individual predictors <script type="math/tex">X_{1}X_{2}..X_{n}</script></li>
  <li><script type="math/tex">\{X_{1},X_{2},...,X_{n}\}</script>: Set of all the individual predictors</li>
  <li>Y: Random variable to be predicted</li>
  <li>y: A particular outcome of Y.</li>
</ul>

<h2 id="model">Model</h2>

<script type="math/tex; mode=display">S_{max}(\{X_{1},X_{2},...,X_{n}\};Y) = I(X; Y) - \sum_{y \in Y} p(Y=y) \max_{i} KL \big[\ P(A_{i} | y) \Vert P(A_{i}) \big]\</script>

<script type="math/tex; mode=display">I(\mathbb{A}_{i};Y=y) = \sum_{a_{i} \in \mathbb{A}_{i}} P(a_{i} | y) \log  \frac{P(a_{i},y)}{P(a_{i})P(y)} = KL \big[\ P(\mathbb{A}_{i} | y) \Vert P(\mathbb{A}_{i}) \big]\</script>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">i_max</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">log_var</span><span class="p">):</span>

    <span class="n">mu_syn</span> <span class="o">=</span> <span class="n">mu</span><span class="p">[:,</span> <span class="n">indices</span><span class="p">]</span>
    <span class="n">log_var_syn</span> <span class="o">=</span> <span class="n">log_var</span><span class="p">[:,</span> <span class="n">indices</span><span class="p">]</span>
    <span class="n">i_max</span> <span class="o">=</span> <span class="n">kl_div</span><span class="p">(</span><span class="n">mu_syn</span><span class="p">,</span> <span class="n">log_var_syn</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">i_max</span>
</code></pre></div></div>

<script type="math/tex; mode=display">\begin{equation}
KL \big[\ q_{\phi}(z_{2}z_{5}z_{8} | x) \Vert p(z_{2}z_{5}z_{8}) \big]\
\end{equation}</script>

<script type="math/tex; mode=display">\mathcal{L}_{elbo}(\theta,\phi,x) =  E_{q_{\phi}(z | x)} \big[\ \log p_{\theta}(x | z) \big]\ - KL \big[\ q_{\phi}(z | x) \Vert p(z) \big]\</script>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\mathcal{L}_{new}(\theta,\phi,x) &= \frac{1}{N}\sum^{N}_{i=1} \bigg[\ E_{q_{\phi}(z | x)} \big[\ \log p_{\theta}(x^{(i)} | z) \big]\ \bigg]\ - KL \big[\ q_{\phi}(z_{n}) \Vert p(z_{n}) \big]\ - I(x_{n};z) \nonumber \\
& \underbrace{- \alpha I(x_{n};z)}_\text{Penalise} + \alpha \sum_{x \in X} p(X=x) \max_{i} KL \big[\ q_{\phi}(\mathbb{A}_{i} | x){p(\mathbb{A}_{i})}) 
\end{align} %]]></script>

<script type="math/tex; mode=display">\mathcal{L}_{new}( \theta,\phi,x ) =  \underbrace{E_{q_{\phi}(z | x)} \big[\ \log p_{\theta}( x | z ) \big]\ - KL \big[\ q_{\phi}( z | x) \Vert p (z)\big]\ }_{\mathcal{L}_{elbo}}- \underbrace{\alpha KL \big[\ q_{\phi}(\mathbb{A}_{worst} | x) \Vert p(\mathbb{A}_{worst})\big]\ }_{\alpha*\text{Imax}}</script>

<p><img src="http://localhost:4000/images/traversal_mean_white.png" alt="results" /></p>

<h2 id="references">References:</h2>

<ul>
  <li>
    <p>Irina Higgins, Loƒ±c Matthey, Xavier Glorot, Arka Pal, Benigno Uria, Charles Blundell, Shakir Mohamed,  and Alexander Lerchner.  Early visual concept learning with unsupervised deep learning.CoRR, abs/1606.05579, 2016</p>
  </li>
  <li>
    <p>Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, and Samuel J. Gershman.   Building machines that learn and think like people.CoRR, abs/1604.00289, 2016. URL http://arxiv.org/abs/1604.00289.</p>
  </li>
  <li>
    <p>Loic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner.  dsprites:  Disentanglement testing sprites dataset. https://github.com/deepmind/dsprites-dataset/, 2017.</p>
  </li>
  <li>
    <p>Diederik P. Kingma and Max Welling.   Auto-encoding variational bayes.CoRR, abs/1312.6114,2013. URL http://arxiv.org/abs/1312.6114.</p>
  </li>
  <li>
    <p>Yoshua Bengio, Aaron C. Courville, and Pascal Vincent.   Representation learning:  A review and new perspectives.IEEE Trans. Pattern Anal. Mach. Intell.,  35(8):1798‚Äì1828,  2013.</p>
  </li>
  <li>
    <p>Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra.   Stochastic backpropagation and approximate inference in deep generative models.  In Proceedings of the 31th International Conference on Machine Learning, ICML 2014. URL http://jmlr.org/proceedings/papers/v32/rezende14.html.</p>
  </li>
</ul>
:ET